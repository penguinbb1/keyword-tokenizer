#!/usr/bin/env python
"""
ä» AI æ ‡æ³¨ç»“æœå¯¼å…¥è¯å…¸

å°† test_results_ai.json ä¸­ AI æ ‡æ³¨çš„é«˜è´¨é‡è¯æ±‡å¯¼å…¥åˆ°è¯å…¸ä¸­ï¼Œ
è¿™æ ·ä¸‹æ¬¡å¤„ç†æ—¶å°±ä¸éœ€è¦å†è°ƒç”¨ AIã€‚

ç”¨æ³•:
    python scripts/import_ai_tags.py test_results_ai.json [--dry-run]
"""
import json
import sys
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# æ ‡ç­¾åˆ°è¯å…¸æ–‡ä»¶çš„æ˜ å°„
TAG_TO_DICT = {
    "å“ç‰Œè¯": "brands/global.json",
    "å•†å“è¯": "products.json",
    "äººç¾¤è¯": "audiences.json",
    "åœºæ™¯è¯": "scenarios.json",
    "é¢œè‰²è¯": "colors.json",
    "å°ºå¯¸è¯": "attributes.json",  # å°ºå¯¸è¯æ”¾ attributes
    "å–ç‚¹è¯": "features.json",
    "å±æ€§è¯": "attributes.json",
    "æè´¨è¯": "attributes.json",
    "æ•°é‡è¯": "attributes.json",
    "æ—¶é—´è¯": "attributes.json",
    "å­£èŠ‚è¯": "scenarios.json",
    "åŠ¨ä½œè¯": "attributes.json",
}

# è¦è¿‡æ»¤çš„è¯ï¼ˆè™šè¯ç¢ç‰‡ã€å¤ªçŸ­ã€æˆ–æ˜æ˜¾é”™è¯¯ï¼‰
SKIP_WORDS = {
    # è™šè¯ç¢ç‰‡ï¼ˆè¥¿ç­ç‰™è¯­/æ³•è¯­ï¼‰
    'ni', 'as', 'os', 'ba', 'en', 'es', 'de', 'le', 'la', 'et', 'un', 'une',
    # å¾·è¯­ç¢ç‰‡
    'gr', 'rer', 'wei', 'gro',
    # æ—¥è¯­ç¢ç‰‡
    'ã•ã‚', 'ãã‚', 'ãŸãŸã¿', 'ã›ã‚‹', 'ã¤ã‘ã‚‹', 'ãã„', 'ã‘ã‚‹', 'ãªã„',
    # å¤ªçŸ­æˆ–æ— æ„ä¹‰
    'up', 'to', 'in', 'on', 'an', 'or', 'at', 'by', 'so', 'do', 'go', 'if',
}

# æœ€ä½ç½®ä¿¡åº¦è¦æ±‚
MIN_CONFIDENCE = 0.75

# æœ€ä½å‡ºç°æ¬¡æ•°è¦æ±‚
MIN_COUNT = 2


def load_results(json_path: str) -> Tuple[Dict[str, List[Tuple[str, float]]], Dict[str, int]]:
    """
    ä»æµ‹è¯•ç»“æœä¸­æå– AI æ ‡æ³¨çš„è¯
    
    Returns:
        ai_tagged: {tag: [(word, confidence), ...]}
        word_counts: {word: count}
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    ai_tagged = defaultdict(list)
    word_counts = defaultdict(int)
    seen_words = set()
    
    for result in data.get('results', []):
        for token in result.get('tagged_tokens', []):
            if token.get('method') == 'ai':
                word = token.get('token', '').strip()
                tag = token.get('tags', ['å±æ€§è¯'])[0]
                conf = token.get('confidence', 0.7)
                
                word_counts[word] += 1
                
                # åªè®°å½•ç¬¬ä¸€æ¬¡å‡ºç°
                if word not in seen_words:
                    ai_tagged[tag].append((word, conf))
                    seen_words.add(word)
    
    return dict(ai_tagged), dict(word_counts)


def filter_words(
    ai_tagged: Dict[str, List[Tuple[str, float]]],
    word_counts: Dict[str, int]
) -> Dict[str, List[Tuple[str, float]]]:
    """è¿‡æ»¤æ‰ä½è´¨é‡çš„è¯"""
    filtered = {}
    
    for tag, words in ai_tagged.items():
        good_words = []
        for word, conf in words:
            # è·³è¿‡æ¡ä»¶
            if word.lower() in SKIP_WORDS:
                continue
            if len(word) < 2:
                continue
            if conf < MIN_CONFIDENCE:
                continue
            if word_counts.get(word, 0) < MIN_COUNT:
                continue
            
            good_words.append((word, conf))
        
        if good_words:
            filtered[tag] = good_words
    
    return filtered


def load_existing_dict(dict_path: Path) -> Tuple[dict, set]:
    """åŠ è½½ç°æœ‰è¯å…¸"""
    if dict_path.exists():
        with open(dict_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        existing_words = {e.get('word', '').lower() for e in data.get('entries', [])}
        return data, existing_words
    return {"entries": []}, set()


def save_dict(dict_path: Path, data: dict):
    """ä¿å­˜è¯å…¸"""
    dict_path.parent.mkdir(parents=True, exist_ok=True)
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def import_to_dicts(
    filtered: Dict[str, List[Tuple[str, float]]],
    dict_base: Path,
    dry_run: bool = False
) -> Dict[str, int]:
    """å¯¼å…¥åˆ°è¯å…¸"""
    stats = defaultdict(int)
    
    # æŒ‰ç›®æ ‡è¯å…¸åˆ†ç»„
    by_dict = defaultdict(list)
    for tag, words in filtered.items():
        dict_file = TAG_TO_DICT.get(tag, "attributes.json")
        for word, conf in words:
            by_dict[dict_file].append({
                "word": word,
                "confidence": round(conf, 2),
                "source": "ai_generated",
                "original_tag": tag
            })
    
    # å¯¼å…¥å„è¯å…¸
    for dict_file, new_entries in by_dict.items():
        dict_path = dict_base / dict_file
        data, existing_words = load_existing_dict(dict_path)
        
        added = 0
        for entry in new_entries:
            word = entry["word"]
            if word.lower() not in existing_words:
                # ç®€åŒ– entryï¼Œåªä¿ç•™å¿…è¦å­—æ®µ
                clean_entry = {
                    "word": word,
                    "confidence": entry["confidence"]
                }
                data["entries"].append(clean_entry)
                existing_words.add(word.lower())
                added += 1
        
        if added > 0:
            if not dry_run:
                save_dict(dict_path, data)
                print(f"  âœ“ {dict_file}: æ·»åŠ  {added} æ¡")
            else:
                print(f"  [é¢„è§ˆ] {dict_file}: å°†æ·»åŠ  {added} æ¡")
            
            stats[dict_file] = added
    
    return dict(stats)


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python import_ai_tags.py <test_results.json> [--dry-run]")
        print("ç¤ºä¾‹: python import_ai_tags.py test_results_ai.json --dry-run")
        sys.exit(1)
    
    json_path = sys.argv[1]
    dry_run = '--dry-run' in sys.argv or '-n' in sys.argv
    
    print("=" * 60)
    print("ä» AI æ ‡æ³¨ç»“æœå¯¼å…¥è¯å…¸")
    print("=" * 60)
    
    if dry_run:
        print("\n*** é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶ ***\n")
    
    # è¯å…¸ç›®å½•
    dict_base = Path(__file__).parent.parent / "dictionaries"
    print(f"è¯å…¸ç›®å½•: {dict_base}")
    
    # åŠ è½½ AI æ ‡æ³¨ç»“æœ
    print(f"\nğŸ“‚ åŠ è½½ AI æ ‡æ³¨ç»“æœ: {json_path}")
    ai_tagged, word_counts = load_results(json_path)
    
    total_words = sum(len(v) for v in ai_tagged.values())
    print(f"   AI æ ‡æ³¨è¯æ€»æ•°: {total_words}")
    
    # æŒ‰æ ‡ç­¾ç»Ÿè®¡
    print(f"\nğŸ“Š å„æ ‡ç­¾è¯æ•°:")
    for tag in sorted(ai_tagged.keys()):
        print(f"   {tag}: {len(ai_tagged[tag])} è¯")
    
    # è¿‡æ»¤
    print(f"\nğŸ” è¿‡æ»¤ä½è´¨é‡è¯...")
    print(f"   æœ€ä½ç½®ä¿¡åº¦: {MIN_CONFIDENCE}")
    print(f"   æœ€ä½å‡ºç°æ¬¡æ•°: {MIN_COUNT}")
    print(f"   è·³è¿‡è¯æ•°: {len(SKIP_WORDS)}")
    
    filtered = filter_words(ai_tagged, word_counts)
    filtered_total = sum(len(v) for v in filtered.values())
    print(f"   è¿‡æ»¤åå‰©ä½™: {filtered_total} è¯")
    
    # æ˜¾ç¤ºè¿‡æ»¤åå„æ ‡ç­¾
    print(f"\nğŸ“‹ è¿‡æ»¤åå„æ ‡ç­¾è¯æ•°:")
    for tag in sorted(filtered.keys()):
        words = filtered[tag]
        print(f"   {tag}: {len(words)} è¯")
        # æ˜¾ç¤ºå‰ 5 ä¸ª
        for word, conf in words[:5]:
            count = word_counts.get(word, 0)
            print(f"      - {word} ({conf}, {count}æ¬¡)")
    
    # å¯¼å…¥
    print(f"\nğŸ“ å¯¼å…¥è¯å…¸...")
    stats = import_to_dicts(filtered, dict_base, dry_run)
    
    # æ€»ç»“
    total_added = sum(stats.values())
    print(f"\n" + "=" * 60)
    print(f"æ€»è®¡: {'å°†æ·»åŠ ' if dry_run else 'å·²æ·»åŠ '} {total_added} ä¸ªè¯åˆ°è¯å…¸")
    
    if dry_run:
        print("\nè¦å®é™…å¯¼å…¥ï¼Œè¯·å»æ‰ --dry-run å‚æ•°é‡æ–°è¿è¡Œ")
    else:
        print("\nâœ… å¯¼å…¥å®Œæˆï¼ä¸‹æ¬¡è¿è¡Œæµ‹è¯•æ—¶å°†ä¸éœ€è¦ AI æ ‡æ³¨è¿™äº›è¯ã€‚")


if __name__ == "__main__":
    main()
