#!/usr/bin/env python
"""
æ‰¹é‡æµ‹è¯•è„šæœ¬
ä» CSV æ–‡ä»¶è¯»å–å…³é”®è¯ï¼Œè°ƒç”¨ API è¿›è¡Œåˆ†è¯å’Œæ ‡ç­¾æ ‡æ³¨æµ‹è¯•
"""
import csv
import json
import asyncio
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.enhanced_pipeline import EnhancedPipeline as TokenizePipeline
from services.dictionary_manager import DictionaryManager
from config import settings


def detect_encoding(file_path: str) -> str:
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin-1', 'shift-jis', 'cp1252']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                f.read()
            return encoding
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    return 'utf-8'  # é»˜è®¤


def load_keywords_from_csv(csv_path: str) -> list:
    """ä» CSV æ–‡ä»¶åŠ è½½å…³é”®è¯"""
    keywords = []
    
    # æ£€æµ‹ç¼–ç 
    encoding = detect_encoding(csv_path)
    print(f"   æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")
    
    with open(csv_path, 'r', encoding=encoding) as f:
        # å°è¯•æ£€æµ‹åˆ†éš”ç¬¦
        sample = f.read(1024)
        f.seek(0)
        
        if '\t' in sample:
            delimiter = '\t'
        else:
            delimiter = ','
        
        print(f"   æ£€æµ‹åˆ°åˆ†éš”ç¬¦: {'TAB' if delimiter == '\t' else 'COMMA'}")
        
        reader = csv.DictReader(f, delimiter=delimiter)
        
        for row in reader:
            # æ”¯æŒä¸åŒçš„åˆ—å
            keyword = row.get('search_term') or row.get('keyword') or row.get('å…³é”®è¯')
            language = row.get('language') or row.get('è¯­è¨€') or 'unknown'
            
            if keyword:
                keywords.append({
                    'keyword': keyword.strip(),
                    'language': language.strip()
                })
    
    return keywords


async def test_single_keyword(pipeline: TokenizePipeline, keyword: str, language: str) -> dict:
    """æµ‹è¯•å•ä¸ªå…³é”®è¯"""
    try:
        result = await pipeline.process(keyword)
        return {
            'keyword': keyword,
            'language': language,
            'success': True,
            'tokens': result.get('tokens', []),
            'tagged_tokens': result.get('tagged_tokens', []),
            'tag_summary': result.get('tag_summary', {})
        }
    except Exception as e:
        return {
            'keyword': keyword,
            'language': language,
            'success': False,
            'error': str(e)
        }


async def run_batch_test(csv_path: str, output_path: str = None):
    """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
    print("=" * 60)
    print("å…³é”®è¯åˆ‡è¯ä¸æ ‡ç­¾æ ‡æ³¨ - æ‰¹é‡æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½å…³é”®è¯
    print(f"\nğŸ“‚ åŠ è½½å…³é”®è¯æ–‡ä»¶: {csv_path}")
    keywords = load_keywords_from_csv(csv_path)
    print(f"   å…± {len(keywords)} æ¡å…³é”®è¯")
    
    # æŒ‰è¯­è¨€ç»Ÿè®¡
    lang_counts = {}
    for kw in keywords:
        lang = kw['language']
        lang_counts[lang] = lang_counts.get(lang, 0) + 1
    
    print("\nğŸ“Š è¯­è¨€åˆ†å¸ƒ:")
    for lang, count in sorted(lang_counts.items()):
        print(f"   {lang}: {count} æ¡")
    
    # åˆå§‹åŒ–å¤„ç†ç®¡é“
    print("\nâš™ï¸ åˆå§‹åŒ–å¤„ç†ç®¡é“...")
    dict_manager = DictionaryManager(settings.dictionary_path)
    dict_manager.load_all()
    pipeline = TokenizePipeline(dict_manager)
    
    # æ‰§è¡Œæµ‹è¯•
    print("\nğŸš€ å¼€å§‹æµ‹è¯•...")
    results = []
    success_count = 0
    
    for i, kw in enumerate(keywords):
        result = await test_single_keyword(pipeline, kw['keyword'], kw['language'])
        results.append(result)
        
        if result['success']:
            success_count += 1
        
        # æ‰“å°è¿›åº¦
        if (i + 1) % 10 == 0 or i == len(keywords) - 1:
            print(f"   è¿›åº¦: {i + 1}/{len(keywords)} ({(i+1)/len(keywords)*100:.1f}%)")
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœç»Ÿè®¡")
    print("=" * 60)
    print(f"æ€»æ•°: {len(results)}")
    print(f"æˆåŠŸ: {success_count}")
    print(f"å¤±è´¥: {len(results) - success_count}")
    print(f"æˆåŠŸç‡: {success_count/len(results)*100:.1f}%")
    
    # æŒ‰è¯­è¨€ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    print("\nğŸ“Š å„è¯­è¨€æ ‡ç­¾åˆ†å¸ƒ:")
    lang_tags = {}
    for result in results:
        if result['success']:
            lang = result['language']
            if lang not in lang_tags:
                lang_tags[lang] = {}
            
            for tag, tokens in result.get('tag_summary', {}).items():
                lang_tags[lang][tag] = lang_tags[lang].get(tag, 0) + len(tokens)
    
    for lang, tags in sorted(lang_tags.items()):
        print(f"\n   ã€{lang}ã€‘")
        for tag, count in sorted(tags.items(), key=lambda x: -x[1]):
            print(f"      {tag}: {count}")
    
    # æ‰“å°ä¸€äº›ç¤ºä¾‹ç»“æœ
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ç»“æœï¼ˆæ¯ç§è¯­è¨€æ˜¾ç¤º2ä¸ªï¼‰")
    print("=" * 60)
    
    shown_langs = {}
    for result in results:
        if result['success']:
            lang = result['language']
            if lang not in shown_langs:
                shown_langs[lang] = 0
            
            if shown_langs[lang] < 2:
                print(f"\nã€{lang}ã€‘{result['keyword']}")
                print(f"   åˆ†è¯: {result['tokens']}")
                print(f"   æ ‡ç­¾: ", end="")
                tag_parts = []
                for tt in result['tagged_tokens']:
                    tag_parts.append(f"{tt['token']}({','.join(tt['tags'])})")
                print(" | ".join(tag_parts))
                shown_langs[lang] += 1
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"test_results_{timestamp}.json"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'summary': {
                'total': len(results),
                'success': success_count,
                'failed': len(results) - success_count,
                'success_rate': success_count / len(results),
                'language_distribution': lang_counts,
                'tag_distribution': lang_tags
            },
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡æµ‹è¯•å…³é”®è¯åˆ†è¯ä¸æ ‡ç­¾æ ‡æ³¨')
    parser.add_argument('csv_file', help='CSV æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„', default=None)
    
    args = parser.parse_args()
    
    if not Path(args.csv_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
        sys.exit(1)
    
    asyncio.run(run_batch_test(args.csv_file, args.output))


if __name__ == "__main__":
    main()
