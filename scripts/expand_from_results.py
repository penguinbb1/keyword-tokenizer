#!/usr/bin/env python3
"""
ä»æµ‹è¯•ç»“æœä¸­æå–é«˜é¢‘ä½ç½®ä¿¡åº¦è¯ï¼Œæ‰©å……è¯å…¸

ä½¿ç”¨æ–¹æ³•:
    python scripts/expand_from_results.py test_results.json
"""
import json
import sys
from pathlib import Path
from collections import Counter

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
DICT_PATH = PROJECT_ROOT / "dictionaries"


def load_results(filepath):
    """åŠ è½½æµ‹è¯•ç»“æœ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def collect_low_conf_words(data, min_count=3):
    """æ”¶é›†é«˜é¢‘ä½ç½®ä¿¡åº¦è¯"""
    by_language = {}
    
    for result in data.get('results', []):
        lang = result.get('language', 'unknown')
        if lang not in by_language:
            by_language[lang] = Counter()
        
        for token in result.get('tagged_tokens', []):
            if token.get('confidence', 0) <= 0.5:
                word = token.get('token', '')
                if len(word) > 1:
                    by_language[lang][word] += 1
    
    # è¿‡æ»¤ä½é¢‘è¯
    for lang in by_language:
        by_language[lang] = {
            w: c for w, c in by_language[lang].items() 
            if c >= min_count
        }
    
    return by_language


def categorize_japanese(words):
    """åˆ†ç±»æ—¥è¯­è¯"""
    categories = {
        'products': [],
        'scenarios': [],
        'features': [],
        'attributes': [],
    }
    
    product_suffixes = ['ãƒªãƒ¥ãƒƒã‚¯', 'ãƒãƒƒã‚°', 'ã‚·ãƒ¥ãƒ¼ã‚º', 'ãƒ™ã‚¹ãƒˆ', 'ãƒ‘ãƒ³ãƒ„', 
                        'ã‚¶ãƒƒã‚¯', 'ãƒãƒ¼ãƒ', 'ã‚±ãƒ¼ã‚¹', 'ãƒœãƒˆãƒ«', 'ã‚¸ãƒ£ã‚±ãƒƒãƒˆ',
                        'ã‚³ãƒ¼ãƒˆ', 'ã‚·ãƒ£ãƒ„']
    scenario_prefixes = ['ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°', 'ãƒã‚¤ã‚­ãƒ³ã‚°', 'ãƒˆãƒ¬ãƒƒã‚­ãƒ³ã‚°', 'ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢',
                         'ã‚­ãƒ£ãƒ³ãƒ—', 'ãƒˆãƒ¬ã‚¤ãƒ«', 'ãƒãƒ©ã‚½ãƒ³', 'ã‚¸ãƒ§ã‚®ãƒ³ã‚°', 'ã‚¦ã‚©ãƒ¼ã‚­ãƒ³ã‚°']
    
    for word, count in words.items():
        # å•†å“è¯ï¼ˆåŒ…å«å•†å“åç¼€ï¼‰
        if any(suffix in word for suffix in product_suffixes):
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤åˆå•†å“è¯ï¼ˆåœºæ™¯+å•†å“ï¼‰
            categories['products'].append({
                'word': word,
                'count': count,
                'confidence': 0.85
            })
        # è·³è¿‡ä¸€äº›å™ªéŸ³è¯
        elif word in ['ä»˜ã', 'ã‚Œãªã„', 'å¤šã„', 'é€šã›ã‚‹', 'è»½ã„']:
            categories['attributes'].append({
                'word': word,
                'count': count,
                'confidence': 0.7
            })
    
    return categories


def categorize_spanish(words):
    """åˆ†ç±»è¥¿ç­ç‰™è¯­è¯"""
    categories = {
        'products': [],
        'scenarios': [],
        'features': [],
        'attributes': [],
        'colors': [],
    }
    
    # é¢„å®šä¹‰åˆ†ç±»
    product_words = {'molde', 'estuche', 'coche', 'juego', 'bolsa', 'bocina'}
    material_words = {'madera', 'acero', 'silicona', 'agua'}
    feature_words = {'electrica', 'electrico', 'remoto', 'expandible', 
                     'muscular', 'interior', 'presion'}
    body_words = {'nariz', 'juanete', 'fascitis', 'cuello'}
    scenario_words = {'jardin', 'navidad', 'wc', 'emergencia'}
    
    for word, count in words.items():
        entry = {'word': word, 'count': count, 'confidence': 0.85}
        
        if word in product_words:
            categories['products'].append(entry)
        elif word in material_words:
            categories['attributes'].append(entry)
        elif word in feature_words:
            categories['features'].append(entry)
        elif word in body_words:
            categories['attributes'].append(entry)
        elif word in scenario_words:
            categories['scenarios'].append(entry)
        # å¦‚æœè¯ä»¥ -o/-a ç»“å°¾ï¼Œå¯èƒ½æ˜¯å½¢å®¹è¯
        elif word.endswith('o') or word.endswith('a'):
            categories['attributes'].append(entry)
    
    return categories


def categorize_german(words):
    """åˆ†ç±»å¾·è¯­è¯"""
    categories = {
        'products': [],
        'features': [],
        'attributes': [],
    }
    
    feature_words = {'gefÃ¼ttert', 'wasserdicht', 'atmungsaktiv'}
    
    for word, count in words.items():
        entry = {'word': word, 'count': count, 'confidence': 0.85}
        
        if word in feature_words:
            categories['features'].append(entry)
        else:
            # å¾·è¯­å¤åˆè¯é€šå¸¸æ˜¯å•†å“æˆ–å±æ€§
            categories['attributes'].append(entry)
    
    return categories


def categorize_french(words):
    """åˆ†ç±»æ³•è¯­è¯"""
    categories = {
        'products': [],
        'features': [],
        'attributes': [],
    }
    
    for word, count in words.items():
        entry = {'word': word, 'count': count, 'confidence': 0.85}
        categories['attributes'].append(entry)
    
    return categories


def update_dictionary(dict_name, new_entries, dry_run=True):
    """æ›´æ–°è¯å…¸æ–‡ä»¶"""
    dict_file = DICT_PATH / f"{dict_name}.json"
    
    if not dict_file.exists():
        print(f"  âš ï¸ è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨: {dict_file}")
        return 0
    
    with open(dict_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    existing_words = {e.get('word', '').lower() for e in data.get('entries', [])}
    
    added = 0
    for entry in new_entries:
        word = entry['word']
        if word.lower() not in existing_words:
            data['entries'].append({
                'word': word,
                'confidence': entry.get('confidence', 0.85)
            })
            added += 1
            if not dry_run:
                existing_words.add(word.lower())
    
    if not dry_run and added > 0:
        with open(dict_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    return added


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python scripts/expand_from_results.py <results.json> [--apply]")
        sys.exit(1)
    
    results_file = sys.argv[1]
    dry_run = '--apply' not in sys.argv
    
    if dry_run:
        print("ğŸ” é¢„è§ˆæ¨¡å¼ï¼ˆæ·»åŠ  --apply å®é™…æ‰§è¡Œï¼‰\n")
    else:
        print("âš¡ æ‰§è¡Œæ¨¡å¼\n")
    
    # åŠ è½½ç»“æœ
    data = load_results(results_file)
    print(f"ğŸ“‚ å·²åŠ è½½ {len(data.get('results', []))} æ¡ç»“æœ\n")
    
    # æ”¶é›†ä½ç½®ä¿¡åº¦è¯
    by_language = collect_low_conf_words(data, min_count=3)
    
    # æŒ‰è¯­è¨€å¤„ç†
    total_added = 0
    
    for lang, words in by_language.items():
        if not words:
            continue
        
        print(f"=== {lang} ({len(words)} ä¸ªé«˜é¢‘ä½ç½®ä¿¡åº¦è¯) ===")
        
        # åˆ†ç±»
        if lang == 'æ—¥è¯­':
            categories = categorize_japanese(words)
        elif lang == 'è¥¿ç­ç‰™è¯­':
            categories = categorize_spanish(words)
        elif lang == 'å¾·è¯­':
            categories = categorize_german(words)
        elif lang == 'æ³•è¯­':
            categories = categorize_french(words)
        else:
            continue
        
        # æ›´æ–°è¯å…¸
        for cat, entries in categories.items():
            if not entries:
                continue
            
            count = update_dictionary(cat, entries, dry_run)
            if count > 0:
                print(f"  {cat}: +{count} è¯")
                for e in entries[:5]:
                    print(f"    - {e['word']} ({e['count']}æ¬¡)")
                if len(entries) > 5:
                    print(f"    ... è¿˜æœ‰ {len(entries) - 5} è¯")
            total_added += count
        
        print()
    
    print(f"{'é¢„è®¡' if dry_run else 'å·²'}æ·»åŠ  {total_added} ä¸ªè¯")
    
    if dry_run:
        print("\nğŸ’¡ ä½¿ç”¨ --apply å‚æ•°å®é™…æ‰§è¡Œæ›´æ–°")


if __name__ == "__main__":
    main()
