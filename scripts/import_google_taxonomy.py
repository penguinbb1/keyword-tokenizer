#!/usr/bin/env python
"""
å¯¼å…¥ Google Product Taxonomy åˆ°è¯å…¸

Google Product Taxonomy æ˜¯ Google å®˜æ–¹çš„å•†å“åˆ†ç±»ä½“ç³»ï¼Œ
åŒ…å« 5000+ å•†å“ç±»ç›®ï¼Œæ”¯æŒ 20+ è¯­è¨€ã€‚

ç”¨æ³•:
    python scripts/import_google_taxonomy.py [--dry-run] [--lang en-US]
    
æ”¯æŒçš„è¯­è¨€:
    en-US (è‹±è¯­), de-DE (å¾·è¯­), fr-FR (æ³•è¯­), 
    es-ES (è¥¿ç­ç‰™è¯­), ja-JP (æ—¥è¯­), zh-CN (ä¸­æ–‡)
"""
import json
import re
import sys
import requests
from pathlib import Path
from collections import defaultdict

# è¯­è¨€ä»£ç æ˜ å°„
LANGUAGE_CODES = {
    'en': 'en-US',
    'de': 'de-DE', 
    'fr': 'fr-FR',
    'es': 'es-ES',
    'ja': 'ja-JP',
    'zh': 'zh-CN',
    'it': 'it-IT',
    'pt': 'pt-BR',
    'nl': 'nl-NL',
    'pl': 'pl-PL',
}

# åœºæ™¯è¯å…³é”®å­—ï¼ˆç”¨äºåˆ†ç±»ï¼‰
SCENARIO_KEYWORDS = {
    'en': {'outdoor', 'indoor', 'sports', 'fitness', 'camping', 'hiking', 
           'swimming', 'running', 'cycling', 'fishing', 'hunting', 'golf',
           'yoga', 'gym', 'travel', 'office', 'home', 'garden', 'kitchen',
           'bathroom', 'bedroom', 'wedding', 'party', 'christmas', 'halloween'},
    'de': {'outdoor', 'indoor', 'sport', 'fitness', 'camping', 'wandern',
           'schwimmen', 'laufen', 'radfahren', 'angeln', 'jagd', 'golf',
           'yoga', 'reise', 'bÃ¼ro', 'haus', 'garten', 'kÃ¼che', 'bad'},
    'fr': {'outdoor', 'intÃ©rieur', 'sport', 'fitness', 'camping', 'randonnÃ©e',
           'natation', 'course', 'cyclisme', 'pÃªche', 'chasse', 'golf',
           'yoga', 'voyage', 'bureau', 'maison', 'jardin', 'cuisine'},
    'es': {'exterior', 'interior', 'deporte', 'fitness', 'camping', 'senderismo',
           'nataciÃ³n', 'correr', 'ciclismo', 'pesca', 'caza', 'golf',
           'yoga', 'viaje', 'oficina', 'hogar', 'jardÃ­n', 'cocina'},
    'ja': {'ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢', 'ã‚¤ãƒ³ãƒ‰ã‚¢', 'ã‚¹ãƒãƒ¼ãƒ„', 'ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹', 'ã‚­ãƒ£ãƒ³ãƒ—',
           'ãƒã‚¤ã‚­ãƒ³ã‚°', 'ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°', 'ã‚µã‚¤ã‚¯ãƒªãƒ³ã‚°', 'é‡£ã‚Š', 'ã‚´ãƒ«ãƒ•',
           'ãƒ¨ã‚¬', 'æ—…è¡Œ', 'ã‚ªãƒ•ã‚£ã‚¹', 'ãƒ›ãƒ¼ãƒ ', 'ã‚¬ãƒ¼ãƒ‡ãƒ³', 'ã‚­ãƒƒãƒãƒ³'},
}

# è¦è·³è¿‡çš„é€šç”¨è¯
SKIP_WORDS = {
    'en': {'&', 'and', 'or', 'the', 'a', 'an', 'for', 'with', 'by', 'to', 'of',
           'in', 'on', 'at', 'as', 'is', 'it', 'be', 'are', 'was', 'were',
           'other', 'all', 'new', 'used', 'general', 'special', 'custom'},
    'de': {'&', 'und', 'oder', 'der', 'die', 'das', 'fÃ¼r', 'mit', 'von', 'zu',
           'in', 'auf', 'an', 'als', 'ist', 'sind', 'war', 'waren',
           'andere', 'alle', 'neu', 'gebraucht', 'allgemein', 'spezial'},
    'fr': {'&', 'et', 'ou', 'le', 'la', 'les', 'pour', 'avec', 'de', 'Ã ',
           'en', 'sur', 'dans', 'comme', 'est', 'sont', 'Ã©tait', 'Ã©taient',
           'autre', 'tous', 'nouveau', 'gÃ©nÃ©ral', 'spÃ©cial'},
    'es': {'&', 'y', 'o', 'el', 'la', 'los', 'las', 'para', 'con', 'de', 'a',
           'en', 'sobre', 'como', 'es', 'son', 'era', 'eran',
           'otro', 'todos', 'nuevo', 'general', 'especial'},
    'ja': {'&', 'ã¨', 'ã‚„', 'ã®', 'ã‚’', 'ã«', 'ã¯', 'ãŒ', 'ã§', 'ã¸',
           'ãã®ä»–', 'ã™ã¹ã¦', 'æ–°å“', 'ä¸­å¤', 'ä¸€èˆ¬', 'ç‰¹æ®Š'},
}


def download_taxonomy(lang_code: str, local_file: str = None) -> list:
    """ä¸‹è½½æˆ–ä»æœ¬åœ°åŠ è½½ Google Product Taxonomy"""
    
    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    if local_file:
        local_path = Path(local_file)
        if local_path.exists():
            print(f"ğŸ“‚ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½: {local_file}")
            return parse_taxonomy_file(local_path)
        else:
            print(f"   âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_file}")
            return []
    
    # æ£€æŸ¥é»˜è®¤æœ¬åœ°æ–‡ä»¶ä½ç½®
    script_dir = Path(__file__).parent
    default_local = script_dir / "taxonomy_data" / f"taxonomy.{lang_code}.txt"
    if default_local.exists():
        print(f"ğŸ“‚ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½: {default_local}")
        return parse_taxonomy_file(default_local)
    
    # å°è¯•ç½‘ç»œä¸‹è½½
    url = f"https://www.google.com/basepages/producttype/taxonomy-with-ids.{lang_code}.txt"
    
    print(f"ğŸ“¥ ä¸‹è½½ Google Taxonomy ({lang_code})...")
    print(f"   URL: {url}")
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        categories = parse_taxonomy_content(response.text)
        print(f"   âœ“ ä¸‹è½½æˆåŠŸï¼Œå…± {len(categories)} ä¸ªåˆ†ç±»")
        return categories
        
    except requests.exceptions.RequestException as e:
        print(f"   âŒ ä¸‹è½½å¤±è´¥: {e}")
        print(f"\nğŸ’¡ è¯·æ‰‹åŠ¨ä¸‹è½½æ–‡ä»¶:")
        print(f"   1. è®¿é—®: {url}")
        print(f"   2. ä¿å­˜åˆ°: {default_local}")
        print(f"   3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        return []


def parse_taxonomy_file(file_path: Path) -> list:
    """è§£ææœ¬åœ° Taxonomy æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    return parse_taxonomy_content(content)


def parse_taxonomy_content(content: str) -> list:
    """è§£æ Taxonomy å†…å®¹"""
    lines = content.strip().split('\n')
    categories = []
    
    for line in lines[1:]:  # è·³è¿‡ç¬¬ä¸€è¡Œæ³¨é‡Š
        line = line.strip()
        if not line:
            continue
        
        # æ ¼å¼1: "1 - Animals & Pet Supplies" (with IDs)
        if ' - ' in line and line.split(' - ')[0].strip().isdigit():
            parts = line.split(' - ', 1)
            if len(parts) == 2:
                categories.append(parts[1].strip())
        # æ ¼å¼2: "Animals & Pet Supplies" (without IDs)
        elif ' > ' in line or (line and not line[0].isdigit()):
            categories.append(line)
    
    return categories


def extract_words(categories: list, lang: str) -> dict:
    """ä»åˆ†ç±»ä¸­æå–è¯æ±‡"""
    product_words = defaultdict(lambda: {'count': 0, 'sources': []})
    scenario_words = defaultdict(lambda: {'count': 0, 'sources': []})
    
    skip = SKIP_WORDS.get(lang, SKIP_WORDS['en'])
    scenarios = SCENARIO_KEYWORDS.get(lang, SCENARIO_KEYWORDS['en'])
    
    for cat in categories:
        # åˆ†å‰²å±‚çº§
        levels = cat.split(' > ')
        
        for level in levels:
            # æå–å•è¯
            if lang == 'ja':
                # æ—¥è¯­ï¼šæŒ‰åŸæ ·ä¿ç•™
                words = [level]
            else:
                # å…¶ä»–è¯­è¨€ï¼šåˆ†è¯
                words = re.findall(r'\b\w+\b', level.lower())
            
            for word in words:
                # è·³è¿‡æ¡ä»¶
                if word in skip:
                    continue
                if len(word) < 2:
                    continue
                if word.isdigit():
                    continue
                
                # åˆ†ç±»ï¼šåœºæ™¯è¯ or å•†å“è¯
                if word in scenarios:
                    scenario_words[word]['count'] += 1
                    if cat not in scenario_words[word]['sources']:
                        scenario_words[word]['sources'].append(cat)
                else:
                    product_words[word]['count'] += 1
                    if cat not in product_words[word]['sources']:
                        product_words[word]['sources'].append(cat)
    
    return {
        'products': dict(product_words),
        'scenarios': dict(scenario_words)
    }


def load_existing_dict(dict_path: Path) -> set:
    """åŠ è½½ç°æœ‰è¯å…¸ï¼Œè·å–å·²æœ‰è¯æ±‡"""
    existing = set()
    
    if dict_path.exists():
        with open(dict_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for entry in data.get('entries', []):
            word = entry.get('word', '').lower()
            if word:
                existing.add(word)
    
    return existing


def merge_to_dict(dict_path: Path, new_words: dict, dry_run: bool = False) -> int:
    """åˆå¹¶æ–°è¯åˆ°è¯å…¸"""
    # åŠ è½½ç°æœ‰è¯å…¸
    if dict_path.exists():
        with open(dict_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = {'entries': []}
    
    existing = {e.get('word', '').lower() for e in data.get('entries', [])}
    
    # æ·»åŠ æ–°è¯
    added = 0
    for word, info in new_words.items():
        if word.lower() not in existing:
            entry = {
                'word': word,
                'confidence': 0.85,  # Google Taxonomy æ¥æºç»™ 0.85
                'source': 'google_taxonomy',
            }
            data['entries'].append(entry)
            existing.add(word.lower())
            added += 1
    
    # ä¿å­˜
    if not dry_run and added > 0:
        with open(dict_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    return added


def main():
    print("=" * 60)
    print("ğŸ“¦ Google Product Taxonomy å¯¼å…¥å·¥å…·")
    print("=" * 60)
    
    # è§£æå‚æ•°
    dry_run = '--dry-run' in sys.argv or '-n' in sys.argv
    
    # è·å–è¯­è¨€å‚æ•°
    lang = 'en'
    for i, arg in enumerate(sys.argv):
        if arg == '--lang' and i + 1 < len(sys.argv):
            lang = sys.argv[i + 1]
            break
    
    # è·å–æœ¬åœ°æ–‡ä»¶å‚æ•°
    local_file = None
    for i, arg in enumerate(sys.argv):
        if arg == '--file' and i + 1 < len(sys.argv):
            local_file = sys.argv[i + 1]
            break
    
    lang_code = LANGUAGE_CODES.get(lang, lang)
    
    if dry_run:
        print("\n*** é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶ ***")
    
    print(f"\nè¯­è¨€: {lang} ({lang_code})")
    
    # è¯å…¸ç›®å½•
    script_dir = Path(__file__).parent
    dict_base = script_dir.parent / "dictionaries"
    
    print(f"è¯å…¸ç›®å½•: {dict_base}")
    
    # ä¸‹è½½æˆ–åŠ è½½åˆ†ç±»
    categories = download_taxonomy(lang_code, local_file)
    if not categories:
        print("âŒ æ— æ³•è·å–åˆ†ç±»æ•°æ®")
        sys.exit(1)
    
    # æå–è¯æ±‡
    print(f"\nğŸ” æå–è¯æ±‡...")
    extracted = extract_words(categories, lang)
    
    product_count = len(extracted['products'])
    scenario_count = len(extracted['scenarios'])
    
    print(f"   å•†å“è¯: {product_count} ä¸ª")
    print(f"   åœºæ™¯è¯: {scenario_count} ä¸ª")
    
    # æ˜¾ç¤ºé«˜é¢‘è¯ç¤ºä¾‹
    print(f"\nğŸ“Š é«˜é¢‘å•†å“è¯ Top 20:")
    sorted_products = sorted(
        extracted['products'].items(), 
        key=lambda x: x[1]['count'], 
        reverse=True
    )[:20]
    for word, info in sorted_products:
        print(f"   {word}: {info['count']}æ¬¡")
    
    print(f"\nğŸ“Š åœºæ™¯è¯ç¤ºä¾‹:")
    for word in list(extracted['scenarios'].keys())[:10]:
        print(f"   {word}")
    
    # åˆå¹¶åˆ°è¯å…¸
    print(f"\nğŸ“ åˆå¹¶åˆ°è¯å…¸...")
    
    # å•†å“è¯
    products_path = dict_base / "products.json"
    products_added = merge_to_dict(products_path, extracted['products'], dry_run)
    action = "å°†æ·»åŠ " if dry_run else "å·²æ·»åŠ "
    print(f"   products.json: {action} {products_added} ä¸ªæ–°è¯")
    
    # åœºæ™¯è¯
    scenarios_path = dict_base / "scenarios.json"
    scenarios_added = merge_to_dict(scenarios_path, extracted['scenarios'], dry_run)
    print(f"   scenarios.json: {action} {scenarios_added} ä¸ªæ–°è¯")
    
    # æ€»ç»“
    total_added = products_added + scenarios_added
    print(f"\n{'=' * 60}")
    print(f"æ€»è®¡: {action} {total_added} ä¸ªè¯åˆ°è¯å…¸")
    
    if dry_run:
        print("\nè¦å®é™…å¯¼å…¥ï¼Œè¯·å»æ‰ --dry-run å‚æ•°é‡æ–°è¿è¡Œ:")
        print(f"  python scripts/import_google_taxonomy.py --lang {lang}")
    else:
        print("\nâœ… å¯¼å…¥å®Œæˆï¼é‡æ–°è¿è¡Œæµ‹è¯•æŸ¥çœ‹æ•ˆæœã€‚")
    
    # æ˜¾ç¤ºæ”¯æŒçš„è¯­è¨€
    print(f"\nğŸ’¡ æ”¯æŒçš„è¯­è¨€:")
    for code, full in LANGUAGE_CODES.items():
        print(f"   --lang {code}  ({full})")


if __name__ == "__main__":
    main()
