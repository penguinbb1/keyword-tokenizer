#!/usr/bin/env python
"""
æ‰¹é‡æµ‹è¯•è„šæœ¬ V2 - ä¼˜åŒ– AI è°ƒç”¨

ä¸¤é˜¶æ®µå¤„ç†ï¼š
1. ç¬¬ä¸€é˜¶æ®µï¼šä¸ç”¨ AIï¼Œå¤„ç†æ‰€æœ‰å…³é”®è¯ï¼Œæ”¶é›†ä½ç½®ä¿¡åº¦è¯
2. ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è°ƒç”¨ AI æ ‡æ³¨ä½ç½®ä¿¡åº¦è¯
3. ç¬¬ä¸‰é˜¶æ®µï¼šåˆå¹¶ç»“æœ

è¿™æ ·å¯ä»¥å¤§å¤§å‡å°‘ API è°ƒç”¨æ¬¡æ•°ï¼ˆä»æ•°åƒæ¬¡å‡å°‘åˆ°å‡ åæ¬¡ï¼‰
"""
import csv
import json
import asyncio
import sys
from pathlib import Path
from datetime import datetime
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.enhanced_pipeline import EnhancedPipeline
from services.dictionary_manager import DictionaryManager
from config import settings


def detect_encoding(file_path: str) -> str:
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin-1', 'shift-jis', 'cp1252']
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                f.read()
            return encoding
        except (UnicodeDecodeError, UnicodeError):
            continue
    return 'utf-8'


def load_keywords_from_csv(csv_path: str) -> list:
    """ä» CSV æ–‡ä»¶åŠ è½½å…³é”®è¯"""
    keywords = []
    encoding = detect_encoding(csv_path)
    print(f"   æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")
    
    with open(csv_path, 'r', encoding=encoding) as f:
        sample = f.read(1024)
        f.seek(0)
        delimiter = '\t' if '\t' in sample else ','
        print(f"   æ£€æµ‹åˆ°åˆ†éš”ç¬¦: {'TAB' if delimiter == '\t' else 'COMMA'}")
        
        reader = csv.DictReader(f, delimiter=delimiter)
        
        # æ‰“å°åˆ—åå¸®åŠ©è°ƒè¯•
        first_row = next(reader, None)
        if first_row:
            print(f"   CSV åˆ—å: {list(first_row.keys())}")
            f.seek(0)
            reader = csv.DictReader(f, delimiter=delimiter)
        
        for row in reader:
            # æ”¯æŒå¤šç§åˆ—å
            keyword = (row.get('search_term') or row.get('keyword') or 
                      row.get('Keyword') or row.get('å…³é”®è¯') or '')
            language = (row.get('language') or row.get('Language') or 
                       row.get('è¯­è¨€') or 'unknown')
            
            if keyword:
                keywords.append({
                    'keyword': keyword.strip(),
                    'language': language.strip()
                })
    
    return keywords


async def phase1_collect_low_conf(pipeline, keywords):
    """
    ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†æ‰€æœ‰å…³é”®è¯ï¼Œæ”¶é›†ä½ç½®ä¿¡åº¦è¯
    """
    results = []
    low_conf_words = Counter()  # ç»Ÿè®¡ä½ç½®ä¿¡åº¦è¯å‡ºç°æ¬¡æ•°
    
    # è¯­è¨€åç§°æ˜ å°„
    lang_map = {
        'æ—¥è¯­': 'ja', 'æ—¥æœ¬èª': 'ja', 'japanese': 'ja',
        'è¥¿ç­ç‰™è¯­': 'es', 'spanish': 'es', 'espaÃ±ol': 'es',
        'å¾·è¯­': 'de', 'german': 'de', 'deutsch': 'de',
        'æ³•è¯­': 'fr', 'french': 'fr', 'franÃ§ais': 'fr',
        'è‹±è¯­': 'en', 'english': 'en',
    }
    
    total = len(keywords)
    
    for i, item in enumerate(keywords):
        if (i + 1) % 500 == 0:
            print(f"   é˜¶æ®µ1è¿›åº¦: {i+1}/{total} ({(i+1)/total*100:.1f}%)")
        
        keyword = item['keyword']
        language = item['language']
        
        # è½¬æ¢è¯­è¨€ä»£ç 
        lang_code = lang_map.get(language.lower(), language.lower()) if language else None
        
        try:
            result = await pipeline.process(keyword, language=lang_code)
            result['language'] = language
            results.append(result)
            
            # æ”¶é›†ä½ç½®ä¿¡åº¦è¯
            for token in result.get('tagged_tokens', []):
                if token.get('confidence', 0) <= 0.5:
                    word = token.get('token', '')
                    if len(word) > 1:  # è·³è¿‡å•å­—ç¬¦
                        low_conf_words[word] += 1
                        
        except Exception as e:
            print(f"   âš ï¸ å¤„ç†å¤±è´¥ [{keyword}]: {e}")
            results.append({
                'keyword': keyword,
                'language': language,
                'tokens': [],
                'tagged_tokens': [],
                'error': str(e)
            })
    
    return results, low_conf_words


async def phase2_ai_batch_tagging(low_conf_words, min_count=2):
    """
    ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡è°ƒç”¨ AI æ ‡æ³¨ä½ç½®ä¿¡åº¦è¯
    
    åªå¤„ç†å‡ºç°æ¬¡æ•° >= min_count çš„è¯ï¼ˆå‡å°‘å™ªéŸ³ï¼‰
    """
    from services.ai_enhancer_v2 import AIEnhancer
    
    enhancer = AIEnhancer()
    
    if not enhancer.is_enabled:
        print("   âš ï¸ AI æœåŠ¡æœªå¯ç”¨ï¼Œè·³è¿‡ AI æ ‡æ³¨")
        return {}
    
    # ç­›é€‰é«˜é¢‘ä½ç½®ä¿¡åº¦è¯
    words_to_tag = [word for word, count in low_conf_words.items() if count >= min_count]
    
    print(f"   å…± {len(words_to_tag)} ä¸ªé«˜é¢‘ä½ç½®ä¿¡åº¦è¯éœ€è¦ AI æ ‡æ³¨")
    
    if not words_to_tag:
        return {}
    
    # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ 50 ä¸ªè¯ï¼‰
    batch_size = 50
    all_results = {}
    
    for i in range(0, len(words_to_tag), batch_size):
        batch = words_to_tag[i:i + batch_size]
        print(f"   AI æ ‡æ³¨æ‰¹æ¬¡ {i//batch_size + 1}/{(len(words_to_tag) + batch_size - 1)//batch_size}: {len(batch)} è¯")
        
        try:
            results = await enhancer.process_batch(batch, context="ç”µå•†å…³é”®è¯")
            all_results.update(results)
            
            # é¿å… API é™æµ
            await asyncio.sleep(1)
        except Exception as e:
            print(f"   âš ï¸ AI æ‰¹é‡æ ‡æ³¨å¤±è´¥: {e}")
    
    return all_results


def phase3_merge_results(results, ai_tags):
    """
    ç¬¬ä¸‰é˜¶æ®µï¼šåˆå¹¶ AI æ ‡æ³¨ç»“æœ
    """
    if not ai_tags:
        return results
    
    updated_count = 0
    
    for result in results:
        for token in result.get('tagged_tokens', []):
            word = token.get('token', '')
            if word in ai_tags:
                ai_result = ai_tags[word]
                token['tags'] = [ai_result['tag']]
                token['confidence'] = ai_result['confidence']
                token['method'] = 'ai'
                updated_count += 1
    
    print(f"   å·²æ›´æ–° {updated_count} ä¸ª token çš„æ ‡æ³¨")
    return results


def compute_statistics(results):
    """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        'total': len(results),
        'success': sum(1 for r in results if 'error' not in r),
        'language_distribution': Counter(),
        'tag_distribution': {},
        'confidence_distribution': Counter(),
    }
    
    for result in results:
        lang = result.get('language', 'unknown')
        stats['language_distribution'][lang] += 1
        
        if lang not in stats['tag_distribution']:
            stats['tag_distribution'][lang] = Counter()
        
        for token in result.get('tagged_tokens', []):
            tag = token.get('tags', ['æœªçŸ¥'])[0]
            conf = round(token.get('confidence', 0), 2)
            
            stats['tag_distribution'][lang][tag] += 1
            stats['confidence_distribution'][conf] += 1
    
    stats['success_rate'] = stats['success'] / stats['total'] if stats['total'] > 0 else 0
    
    # è½¬æ¢ Counter ä¸ºæ™®é€š dict
    stats['language_distribution'] = dict(stats['language_distribution'])
    stats['tag_distribution'] = {k: dict(v) for k, v in stats['tag_distribution'].items()}
    stats['confidence_distribution'] = dict(stats['confidence_distribution'])
    
    return stats


async def main():
    print("=" * 60)
    print("å…³é”®è¯åˆ‡è¯ä¸æ ‡ç­¾æ ‡æ³¨ - æ‰¹é‡æµ‹è¯• V2 (AI ä¼˜åŒ–ç‰ˆ)")
    print("=" * 60)
    
    # è§£æå‚æ•°
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python batch_test_v2.py <csv_file> [-o output.json] [--no-ai]")
        sys.exit(1)
    
    csv_path = sys.argv[1]
    output_path = None
    use_ai = True
    
    for i, arg in enumerate(sys.argv):
        if arg == '-o' and i + 1 < len(sys.argv):
            output_path = sys.argv[i + 1]
        if arg == '--no-ai':
            use_ai = False
    
    if not output_path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"test_results_{timestamp}.json"
    
    # åŠ è½½å…³é”®è¯
    print(f"\nğŸ“‚ åŠ è½½å…³é”®è¯æ–‡ä»¶: {csv_path}")
    keywords = load_keywords_from_csv(csv_path)
    print(f"   å…± {len(keywords)} æ¡å…³é”®è¯")
    
    # è¯­è¨€åˆ†å¸ƒ
    lang_dist = Counter(k['language'] for k in keywords)
    print(f"\nğŸ“Š è¯­è¨€åˆ†å¸ƒ:")
    for lang, count in lang_dist.most_common():
        print(f"   {lang}: {count} æ¡")
    
    # åˆå§‹åŒ– pipelineï¼ˆç¦ç”¨å®æ—¶ AIï¼Œåé¢æ‰¹é‡å¤„ç†ï¼‰
    print(f"\nâš™ï¸ åˆå§‹åŒ–å¤„ç†ç®¡é“...")
    dict_manager = DictionaryManager(settings.dictionary_path)
    dict_manager.load_all()
    pipeline = EnhancedPipeline(dict_manager, enable_ai=False)  # ç¦ç”¨å®æ—¶ AI
    
    # é˜¶æ®µ1ï¼šå¤„ç†æ‰€æœ‰å…³é”®è¯
    print(f"\nğŸ”„ é˜¶æ®µ1: å¤„ç†å…³é”®è¯ï¼ˆä¸ä½¿ç”¨ AIï¼‰...")
    results, low_conf_words = await phase1_collect_low_conf(pipeline, keywords)
    
    print(f"   å¤„ç†å®Œæˆï¼Œå…±å‘ç° {len(low_conf_words)} ä¸ªä¸åŒçš„ä½ç½®ä¿¡åº¦è¯")
    print(f"   é«˜é¢‘ä½ç½®ä¿¡åº¦è¯ (Top 10):")
    for word, count in low_conf_words.most_common(10):
        print(f"      {word}: {count}æ¬¡")
    
    # é˜¶æ®µ2ï¼šAI æ‰¹é‡æ ‡æ³¨
    ai_tags = {}
    if use_ai:
        print(f"\nğŸ¤– é˜¶æ®µ2: AI æ‰¹é‡æ ‡æ³¨...")
        ai_tags = await phase2_ai_batch_tagging(low_conf_words, min_count=2)
        print(f"   AI æ ‡æ³¨å®Œæˆï¼Œå…± {len(ai_tags)} ä¸ªè¯")
    else:
        print(f"\nâ­ï¸ è·³è¿‡ AI æ ‡æ³¨ (--no-ai)")
    
    # é˜¶æ®µ3ï¼šåˆå¹¶ç»“æœ
    print(f"\nğŸ“ é˜¶æ®µ3: åˆå¹¶ç»“æœ...")
    results = phase3_merge_results(results, ai_tags)
    
    # è®¡ç®—ç»Ÿè®¡
    print(f"\nğŸ“Š è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
    stats = compute_statistics(results)
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    total_tokens = sum(stats['confidence_distribution'].values())
    low_conf = sum(c for conf, c in stats['confidence_distribution'].items() if conf <= 0.5)
    high_conf = sum(c for conf, c in stats['confidence_distribution'].items() if conf >= 0.85)
    
    print(f"\nğŸ“ˆ ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    print(f"   æ€» tokens: {total_tokens}")
    if total_tokens > 0:
        print(f"   ä½ç½®ä¿¡åº¦ (â‰¤0.5): {low_conf} ({low_conf/total_tokens*100:.1f}%)")
        print(f"   é«˜ç½®ä¿¡åº¦ (â‰¥0.85): {high_conf} ({high_conf/total_tokens*100:.1f}%)")
    else:
        print(f"   âš ï¸ æ— æ•°æ®")
    
    # ä¿å­˜ç»“æœ
    output = {
        'summary': stats,
        'ai_tags_count': len(ai_tags),
        'results': results
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    # ç¤ºä¾‹ç»“æœ
    print(f"\n" + "=" * 60)
    print("ç¤ºä¾‹ç»“æœ")
    print("=" * 60)
    
    for lang in ['æ—¥è¯­', 'å¾·è¯­', 'æ³•è¯­', 'è‹±è¯­', 'è¥¿ç­ç‰™è¯­']:
        samples = [r for r in results if r.get('language') == lang][:2]
        for s in samples:
            # ä» tokens æˆ– original è·å–å…³é”®è¯
            kw = s.get('original', '')
            if not kw and s.get('tokens'):
                kw = ' '.join(s.get('tokens', []))
            
            tokens_str = ' | '.join([
                f"{t['token']}({t['tags'][0]})" 
                for t in s.get('tagged_tokens', [])[:5]
            ])
            print(f"ã€{lang}ã€‘{kw}")
            print(f"   {tokens_str}")


if __name__ == "__main__":
    asyncio.run(main())
