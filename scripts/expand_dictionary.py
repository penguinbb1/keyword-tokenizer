#!/usr/bin/env python
"""
è¯å…¸æ‰©å……è„šæœ¬
ä»æµ‹è¯•ç»“æœä¸­æå–ä½ç½®ä¿¡åº¦è¯ï¼Œä½¿ç”¨ AI æ‰¹é‡æ ‡æ³¨ï¼Œç„¶åæ›´æ–°è¯å…¸
"""
import json
import asyncio
import sys
from pathlib import Path
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent.parent))

from services.dictionary_manager import DictionaryManager
from config import settings


def extract_low_confidence_words(results_file: str, threshold: float = 0.6) -> dict:
    """
    ä»æµ‹è¯•ç»“æœä¸­æå–ä½ç½®ä¿¡åº¦è¯
    
    Returns:
        {"æ—¥è¯­": ["word1", "word2"], "å¾·è¯­": [...], ...}
    """
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    low_conf_words = defaultdict(set)
    
    for result in data.get('results', []):
        language = result.get('language', 'unknown')
        
        for tagged in result.get('tagged_tokens', []):
            if tagged.get('confidence', 0) <= threshold:
                word = tagged.get('token', '')
                # è¿‡æ»¤æ‰å•å­—ç¬¦å’Œçº¯æ•°å­—
                if len(word) > 1 and not word.isdigit():
                    low_conf_words[language].add(word)
    
    # è½¬æ¢ä¸º list
    return {lang: list(words) for lang, words in low_conf_words.items()}


def create_ai_prompt(words: list, language: str) -> str:
    """åˆ›å»º AI æ ‡æ³¨çš„ prompt"""
    
    tag_descriptions = """
- å“ç‰Œè¯: å•†å“å“ç‰Œåç§°ï¼Œå¦‚ Apple, Nike, åä¸º, Sony, Adidas
- å•†å“è¯: å•†å“å“ç±»åç§°ï¼Œå¦‚ è·‘æ­¥é‹, ç¬”è®°æœ¬ç”µè„‘, Tã‚·ãƒ£ãƒ„(Tæ¤), leggings(æ‰“åº•è£¤), rucksack(èƒŒåŒ…)
- äººç¾¤è¯: ç›®æ ‡ç”¨æˆ·ç¾¤ä½“ï¼Œå¦‚ ç”·å£«, å¥³å£«, å„¿ç«¥, ãƒ¡ãƒ³ã‚º(ç”·æ€§), damen(å¥³å£«), femme(å¥³æ€§), herren(ç”·å£«)
- åœºæ™¯è¯: ä½¿ç”¨åœºæ™¯ï¼Œå¦‚ è¿åŠ¨, åŠå…¬, ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°(è·‘æ­¥), camping(éœ²è¥), outdoor(æˆ·å¤–), hiking(å¾’æ­¥)
- é¢œè‰²è¯: é¢œè‰²æè¿°ï¼Œå¦‚ é»‘è‰², çº¢è‰², schwarz(é»‘), noir(é»‘), black, blanco(ç™½)
- å°ºå¯¸è¯: å°ºå¯¸è§„æ ¼ï¼Œå¦‚ 10.5ç , 14å¯¸, 256GB, 15L, XL, mini
- å–ç‚¹è¯: äº§å“å–ç‚¹ç‰¹æ€§ï¼Œå¦‚ é˜²æ°´, è½»é‡, wasserdicht(é˜²æ°´), impermÃ©able(é˜²æ°´), lightweight
- å±æ€§è¯: äº§å“å±æ€§ç‰¹å¾ï¼Œå¦‚ é•¿è¢–, æè´¨, langarm(é•¿è¢–), thermique(ä¿æš–), rechargeable(å¯å……ç”µ)
"""
    
    # æ¯è¡Œä¸€ä¸ªè¯
    words_text = "\n".join([f"- {w}" for w in words[:50]])  # æœ€å¤š50ä¸ª
    
    prompt = f"""ä½ æ˜¯ç”µå•†å…³é”®è¯åˆ†æä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ {language} è¯è¯­åˆ¤æ–­æœ€åˆé€‚çš„æ ‡ç­¾ç±»å‹ã€‚

## å¯é€‰æ ‡ç­¾ç±»å‹ï¼š
{tag_descriptions}

## å¾…æ ‡æ³¨è¯è¯­ï¼ˆ{language}ï¼‰ï¼š
{words_text}

## è¾“å‡ºè¦æ±‚ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ¯è¡Œä¸€ä¸ªè¯ï¼š
```json
{{
  "è¯è¯­1": {{"tag": "å•†å“è¯", "confidence": 0.9}},
  "è¯è¯­2": {{"tag": "åœºæ™¯è¯", "confidence": 0.85}}
}}
```

æ³¨æ„ï¼š
1. confidence èŒƒå›´ 0.7-0.95ï¼Œè¡¨ç¤ºç¡®ä¿¡ç¨‹åº¦
2. å“ç‰Œè¯é€šå¸¸æ˜¯ä¸“æœ‰åè¯
3. å¦‚æœè¯è¯­æ˜æ˜¾æ˜¯æŸä¸ªç±»åˆ«ï¼Œconfidence ç»™ 0.9+
4. è¯·åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""

    return prompt


def print_manual_prompt(language: str, words: list):
    """æ‰“å°æ‰‹åŠ¨ä½¿ç”¨çš„ promptï¼ˆç”¨äºå¤åˆ¶ç²˜è´´åˆ° Claudeï¼‰"""
    prompt = create_ai_prompt(words, language)
    
    print(f"\n{'='*60}")
    print(f"ã€{language}ã€‘è¯å…¸æ‰©å…… - å…± {len(words)} ä¸ªè¯")
    print(f"{'='*60}")
    print("\nå°†ä»¥ä¸‹å†…å®¹å¤åˆ¶åˆ° Claude å¯¹è¯æ¡†ï¼Œè·å–æ ‡æ³¨ç»“æœï¼š\n")
    print("-" * 40)
    print(prompt)
    print("-" * 40)


def parse_ai_response(response_text: str) -> dict:
    """è§£æ AI è¿”å›çš„ JSON"""
    # æå– JSON éƒ¨åˆ†
    if "```json" in response_text:
        response_text = response_text.split("```json")[1].split("```")[0]
    elif "```" in response_text:
        response_text = response_text.split("```")[1].split("```")[0]
    
    return json.loads(response_text.strip())


def update_dictionaries(tagged_words: dict, language: str, dict_manager: DictionaryManager):
    """æ›´æ–°è¯å…¸"""
    tag_to_dict = {
        "å“ç‰Œè¯": "brands",
        "å•†å“è¯": "products", 
        "äººç¾¤è¯": "audiences",
        "åœºæ™¯è¯": "scenarios",
        "é¢œè‰²è¯": "colors",
        "å–ç‚¹è¯": "features",
        "å±æ€§è¯": "attributes",
        "å°ºå¯¸è¯": "attributes",  # å°ºå¯¸è¯ä¹Ÿæ”¾å±æ€§
    }
    
    added_count = 0
    
    for word, info in tagged_words.items():
        tag = info.get("tag", "å±æ€§è¯")
        confidence = info.get("confidence", 0.8)
        
        dict_name = tag_to_dict.get(tag, "attributes")
        
        # å“ç‰Œè¯éœ€è¦åŒºåˆ†è¯­è¨€
        if dict_name == "brands" and language != "è‹±è¯­":
            lang_code = {
                "æ—¥è¯­": "ja",
                "å¾·è¯­": "de", 
                "æ³•è¯­": "fr",
                "è¥¿ç­ç‰™è¯­": "es",
                "ä¸­æ–‡": "zh"
            }.get(language, "global")
            
            if lang_code != "global":
                dict_name = f"brands_{lang_code}"
        
        dict_manager.add_entry(
            word=word,
            tag=tag,
            confidence=confidence,
            source="ai_generated"
        )
        added_count += 1
    
    return added_count


def interactive_mode():
    """äº¤äº’æ¨¡å¼ï¼šæ‰‹åŠ¨ç²˜è´´ AI ç»“æœ"""
    print("\n" + "=" * 60)
    print("äº¤äº’æ¨¡å¼ - ç²˜è´´ AI æ ‡æ³¨ç»“æœ")
    print("=" * 60)
    
    dict_manager = DictionaryManager(settings.dictionary_path)
    dict_manager.load_all()
    
    while True:
        language = input("\nè¯­è¨€ (æ—¥è¯­/å¾·è¯­/æ³•è¯­/è‹±è¯­/è¥¿ç­ç‰™è¯­/ä¸­æ–‡ï¼Œè¾“å…¥ q é€€å‡º): ").strip()
        if language.lower() == 'q':
            break
        
        print("è¯·ç²˜è´´ AI è¿”å›çš„ JSON ç»“æœï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
        lines = []
        while True:
            line = input()
            if not line:
                break
            lines.append(line)
        
        if not lines:
            continue
        
        try:
            response_text = "\n".join(lines)
            tagged_words = parse_ai_response(response_text)
            
            count = update_dictionaries(tagged_words, language, dict_manager)
            print(f"\nâœ… æˆåŠŸæ·»åŠ  {count} ä¸ªè¯æ¡åˆ°è¯å…¸ï¼")
            
        except Exception as e:
            print(f"\nâŒ è§£æå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¯å…¸æ‰©å……å·¥å…·')
    parser.add_argument('results_file', nargs='?', help='æµ‹è¯•ç»“æœ JSON æ–‡ä»¶')
    parser.add_argument('-t', '--threshold', type=float, default=0.6, 
                        help='ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„è¯éœ€è¦æ ‡æ³¨ï¼ˆé»˜è®¤ 0.6ï¼‰')
    parser.add_argument('-l', '--language', help='åªå¤„ç†æŒ‡å®šè¯­è¨€')
    parser.add_argument('-i', '--interactive', action='store_true',
                        help='äº¤äº’æ¨¡å¼ï¼šæ‰‹åŠ¨ç²˜è´´ AI ç»“æœ')
    parser.add_argument('--apply', help='åº”ç”¨ AI æ ‡æ³¨ç»“æœæ–‡ä»¶åˆ°è¯å…¸')
    
    args = parser.parse_args()
    
    # äº¤äº’æ¨¡å¼
    if args.interactive:
        interactive_mode()
        return
    
    # åº”ç”¨ç»“æœæ–‡ä»¶
    if args.apply:
        print(f"ğŸ“‚ åŠ è½½æ ‡æ³¨ç»“æœ: {args.apply}")
        with open(args.apply, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        dict_manager = DictionaryManager(settings.dictionary_path)
        dict_manager.load_all()
        
        total = 0
        for language, tagged_words in data.items():
            if isinstance(tagged_words, dict):
                count = update_dictionaries(tagged_words, language, dict_manager)
                total += count
                print(f"   {language}: æ·»åŠ  {count} ä¸ª")
        
        print(f"\nâœ… å…±æ·»åŠ  {total} ä¸ªè¯æ¡")
        return
    
    # ä»æµ‹è¯•ç»“æœæå–ä½ç½®ä¿¡åº¦è¯
    if not args.results_file:
        parser.print_help()
        return
    
    if not Path(args.results_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.results_file}")
        return
    
    print(f"ğŸ“‚ åˆ†ææµ‹è¯•ç»“æœ: {args.results_file}")
    print(f"ğŸ“Š ç½®ä¿¡åº¦é˜ˆå€¼: {args.threshold}")
    
    low_conf_words = extract_low_confidence_words(args.results_file, args.threshold)
    
    print(f"\nå‘ç°ä½ç½®ä¿¡åº¦è¯ï¼š")
    total_words = 0
    for lang, words in sorted(low_conf_words.items()):
        print(f"   {lang}: {len(words)} ä¸ª")
        total_words += len(words)
    
    print(f"\n   æ€»è®¡: {total_words} ä¸ªè¯éœ€è¦æ ‡æ³¨")
    
    # è¿‡æ»¤æŒ‡å®šè¯­è¨€
    if args.language:
        if args.language in low_conf_words:
            low_conf_words = {args.language: low_conf_words[args.language]}
        else:
            print(f"âŒ æœªæ‰¾åˆ°è¯­è¨€: {args.language}")
            return
    
    # ç”Ÿæˆæ¯ç§è¯­è¨€çš„ prompt
    print("\n" + "=" * 60)
    print("ç”±äºæœªé…ç½® API Keyï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶ä»¥ä¸‹ prompt åˆ° Claude è·å–æ ‡æ³¨ç»“æœ")
    print("ç„¶åä½¿ç”¨ --interactive æ¨¡å¼ç²˜è´´ç»“æœ")
    print("=" * 60)
    
    for language, words in sorted(low_conf_words.items()):
        if len(words) > 0:
            # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹50ä¸ªï¼‰
            for i in range(0, min(len(words), 100), 50):  # æœ€å¤šå¤„ç†100ä¸ª
                batch = words[i:i+50]
                print_manual_prompt(language, batch)
                
                if len(words) > 50:
                    print(f"\nâš ï¸ {language} è¯å¤ªå¤šï¼Œåªæ˜¾ç¤ºå‰ {min(len(words), 100)} ä¸ª")
    
    print("\n" + "=" * 60)
    print("ä½¿ç”¨æ–¹æ³•ï¼š")
    print("1. å¤åˆ¶ä¸Šé¢çš„ prompt åˆ° Claude å¯¹è¯æ¡†")
    print("2. è·å– JSON ç»“æœåï¼Œè¿è¡Œ: python scripts/expand_dictionary.py -i")
    print("3. æŒ‰æç¤ºç²˜è´´ç»“æœï¼Œè‡ªåŠ¨æ›´æ–°è¯å…¸")
    print("=" * 60)


if __name__ == "__main__":
    main()
